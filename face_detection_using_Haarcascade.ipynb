{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from cv2 import imread\n",
    "from cv2 import imshow\n",
    "from cv2 import CascadeClassifier\n",
    "from cv2 import rectangle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imutils \n",
    "from imutils import paths\n",
    "from skimage.feature import hog, greycomatrix, greycoprops\n",
    "from skimage import data, segmentation, color, filters, exposure, restoration\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.morphology import erosion, dilation, opening, closing, white_tophat\n",
    "from skimage.morphology import black_tophat, skeletonize, convex_hull_image\n",
    "from skimage.morphology import disk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import convolve2d as conv2\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from sklearn import metrics\n",
    "from skimage.data import gravel\n",
    "from skimage.filters import difference_of_gaussians, window\n",
    "from scipy.fftpack import fftn, fftshift\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import recall_score, f1_score, accuracy_score, precision_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from matplotlib.patches import Rectangle\n",
    "import tensorflow as tf\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "\n",
    "# from skimage import data\n",
    "# from skimage.color import rgb2gray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    \n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "    base_path = 'I:/Thesis/dataset/final/train_modi - copy'\n",
    "    categories = ['angry' ,'happy', 'sad', 'surprised']\n",
    "\n",
    "\n",
    "    def bgr2rgb(img):\n",
    "        return cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)\n",
    "    # resize image\n",
    "    def image_to_feature_vector(images, size=(150, 150)):\n",
    "        return cv2.resize(images, size).flatten()\n",
    "\n",
    "\n",
    "    def make(rc_img):\n",
    "        '''\n",
    "        fig, axes = plt.subplots(ncols=3, figsize=(15, 3.5))\n",
    "        ax = axes.ravel()\n",
    "        ax[0] = plt.subplot(1, 3, 1)\n",
    "        ax[1] = plt.subplot(1, 3, 2)\n",
    "        ax[2] = plt.subplot(1, 3, 3)\n",
    "        '''\n",
    "        selem = disk(20)\n",
    "        eroded = erosion(rc_img, selem)\n",
    "        roi = rc_img - eroded\n",
    "\n",
    "\n",
    "        pixels = image_to_feature_vector(roi)\n",
    "\n",
    "        rawImages.append(pixels)                   # features\n",
    "        labels.append(cat)\n",
    "        \n",
    "        '''\n",
    "        ax[0].imshow(rc_img)\n",
    "        ax[0].set_title('Original')\n",
    "        ax[0].axis('off')\n",
    "        \n",
    "        ax[1].imshow(eroded, cmap=plt.cm.gray)\n",
    "        ax[1].set_title('Segmented pic')\n",
    "        ax[1].axis('off')\n",
    "\n",
    "    #     ax[2].hist(out.ravel(),256,[0,256], color='r')\n",
    "    #     ax[2].set_title('Histogram')\n",
    "\n",
    "        ax[2].imshow(roi, cmap=plt.cm.gray)\n",
    "        ax[2].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "        '''\n",
    "\n",
    "    labels = []\n",
    "    rawImages = []\n",
    "    \n",
    "    c,b,f=0,0,0\n",
    "#     c = total number of images\n",
    "#     b = detected human face\n",
    "#     f = human face not found\n",
    "\n",
    "    for cat in categories:\n",
    "\n",
    "        image_dir = os.path.join(base_path, cat)\n",
    "        imagePaths = list(paths.list_images(image_dir))\n",
    "\n",
    "        for (i, imagePath) in enumerate(imagePaths):\n",
    "            c=c+1\n",
    "            '''\n",
    "            fig, axes = plt.subplots(ncols=3, figsize=(15, 3.5))\n",
    "            ax = axes.ravel()\n",
    "            ax[0] = plt.subplot(1, 3, 1)\n",
    "            ax[1] = plt.subplot(1, 3, 2)\n",
    "            ax[2] = plt.subplot(1, 3, 3)\n",
    "            '''\n",
    "\n",
    "            image = cv2.imread(imagePath)\n",
    "#             img = image[:,:,::-1]\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "            # perform face detection\n",
    "            bboxes = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "#             ax[2] = plt.gca()\n",
    "            if list(bboxes):\n",
    "                b=b+1\n",
    "\n",
    "                # print bounding box for each detected face\n",
    "                twidth, theight = 0, 0\n",
    "                for box in bboxes:\n",
    "                    x, y, width, height = box\n",
    "\n",
    "                    if width > twidth and height > theight:\n",
    "                        twidth, theight = width, height\n",
    "                        x2, y2 = x + twidth, y + theight\n",
    "                        # draw a rectangle over the pixels\n",
    "                        roi_gray=gray[y:y2, x:x2]\n",
    "#                         rectangle(gray, (x, y), (x2, y2), (0,255,0), 3)\n",
    "#                         rect = Rectangle((x, y),twidth, theight, fill=False, color='yellow')\n",
    "                    else:\n",
    "                        continue\n",
    "                  \n",
    "                make(roi_gray)\n",
    "                roi_flip = cv2.flip(roi_gray, 1)\n",
    "                make(roi_flip)\n",
    "#                 roi_rotate = cv2.rotate(roi_gray, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
    "#                 roi_rotate = imutils.rotate(roi_gray, angle=90)\n",
    "#                 make(roi_rotate)\n",
    "\n",
    "            else:\n",
    "                f=f+1\n",
    "                \n",
    "        print(f\"{cat}: c={c}, b={b}, f={f}\")\n",
    "    \n",
    "    rawImages = np.array(rawImages)               # show some information on the memory consumed by the raw images matrix and features matrix\n",
    "    labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawImages.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(rawImages, labels, test_size=0.25, random_state=30)\n",
    "#     (train_Feat, test_Feat, train_Labels, test_Labels) = train_test_split(features, labels, test_size=0.25, random_state=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier before tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, name):\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    test_prediction = model.predict(X_test)\n",
    "    train_prediction= model.predict(X_train)\n",
    "    \n",
    "    print(\"\\n\"+ name + \"Classifier: \\n\")\n",
    "\n",
    "    test_matrics = confusion_matrix(y_test, test_prediction)\n",
    "    print(\"Confusion Matrix:\\n\", test_matrics)\n",
    "\n",
    "    test_report = classification_report(y_test, test_prediction)\n",
    "    print(\"Classification Report : \\n\", test_report)\n",
    "    \n",
    "    test_acc = accuracy_score(y_test, test_prediction)                     # test accuracy\n",
    "    train_acc = accuracy_score(y_train, train_prediction)\n",
    "    \n",
    "    print(\"Accuracy of \" + name + \" : \\n\")\n",
    "          \n",
    "    print('Train Accuracy : {:.5f} %'.format(train_acc * 100))\n",
    "    print('Test Accuracy : {:.5f} % \\n'.format(test_acc * 100))\n",
    "          \n",
    "    print('Train Recall : ', recall_score(y_train, train_prediction, average='weighted'))\n",
    "    print(\"Test Recall : \", recall_score(y_test, test_prediction, average='weighted'), '\\n')\n",
    "          \n",
    "    print(\"Test F1 : \", f1_score(y_test, test_prediction, average='weighted'))\n",
    "    print(\"Test precison : \", precision_score(y_test, test_prediction, average='weighted'))\n",
    "    \n",
    "\n",
    "    style.use('fivethirtyeight')\n",
    "    sns.set(style='whitegrid', color_codes=True, font_scale=1.2)    # 'font_scale' = size of label\n",
    "    \n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(test_matrics, annot=True, annot_kws={\"size\": 17}, fmt = \"d\")\n",
    "    plt.show()\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Decision Tree Classifier:............................................................................\n",
    "\n",
    "name = \"Decision Tree\"\n",
    "dt = DecisionTreeClassifier(criterion=\"entropy\", max_depth=5)\n",
    "dt_acc = train_model(dt, name)\n",
    "\n",
    "\n",
    "# Logistic Regression Final:..........................................................................\n",
    "\n",
    "name = \"Logistic Regression\"\n",
    "lr = LogisticRegression()\n",
    "lr_acc = train_model(lr, name)\n",
    "\n",
    "\n",
    "# SVM Classifier:.....................................................................................\n",
    "\n",
    "name = \"SVM\"\n",
    "svm = SVC(kernel='linear')\n",
    "svm_acc = train_model(svm, name)\n",
    "\n",
    "\n",
    "# Random Forest Classifier:..................................................................................\n",
    "\n",
    "name = \"Random Forest\"\n",
    "rn = RandomForestClassifier(n_estimators=90)\n",
    "rn_acc = train_model(rn, name)\n",
    "\n",
    "\n",
    "# Naive Bayes Classifier:........................................................................\n",
    "\n",
    "name = \"Naive Bayes\"\n",
    "nb = GaussianNB()\n",
    "nb_acc = train_model(nb, name)\n",
    "\n",
    "\n",
    "# xgBoost classifier:.........................................................................\n",
    "\n",
    "name = \"xgBoots\"\n",
    "xgb = XGBClassifier(n_estimators=10,objective ='binary:logistic',colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10,)\n",
    "xgb_acc = train_model(xgb, name)\n",
    "\n",
    "\n",
    "# ADABoost Classifier:.......................................\n",
    "\n",
    "name = \" ADABoost\"\n",
    "adab = AdaBoostClassifier()\n",
    "adab_acc = train_model(adab, name)\n",
    "\n",
    "\n",
    "# GradientBoost Classifier:.......................................\n",
    "\n",
    "name = \"Gradient Boost\"\n",
    "gb = GradientBoostingClassifier()\n",
    "gb_acc = train_model(gb, name)\n",
    "\n",
    "# Knn Classifier:..................................................................................\n",
    "\n",
    "name = \"KNN\"\n",
    "neighbors = [5, 7, 9, 19,]\n",
    "k_pred = []\n",
    "\n",
    "for k in neighbors:\n",
    "    print(f\"\\nKNN classifier ( k = {k})\\n\")\n",
    "    knn = KNeighborsClassifier(n_neighbors= k)\n",
    "    knn_acc = train_model(knn, name)\n",
    "    k_pred.append(knn_acc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot...........................................................................................\n",
    "\n",
    "height = [dt_acc*100, lr_acc*100, rn_acc*100, nb_acc*100, svm_acc*100, xgb_acc*100, adab_acc*100,\n",
    "          gb_acc*100]\n",
    "bars = ('Decision tree', 'Logistic','Random', 'Naive Bayes','SVM', 'xGBoost', 'ADABoost',\n",
    "        'GradientBoost')\n",
    "\n",
    "y_pos = np.arange(len(bars))\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.bar(y_pos, height, color=['red', 'pink', 'palegreen', 'green', 'blue', 'yellow', 'brown','magenta'])\n",
    "plt.xticks(y_pos, bars)\n",
    "plt.title('Accuracy Comparision', size = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier after tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, name):\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    print(\"\\n\"+ name + \" Classifier: \\n\")\n",
    "\n",
    "    test_matrics = confusion_matrix(y_test, test_pred)\n",
    "    print(\"Confusion Matrix:\\n\", test_matrics)\n",
    "\n",
    "    test_report = classification_report(y_test, test_pred)\n",
    "    print(\"Classification Report : \\n\", test_report)\n",
    "    \n",
    "    test_acc = accuracy_score(y_test, test_pred)                     # test accuracy\n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    \n",
    "    print(\"Accuracy of \" + name + \" : \\n\")\n",
    "          \n",
    "    print('Train Accuracy : {:.5f} %'.format(train_acc * 100))\n",
    "    print('Test Accuracy : {:.5f} % \\n'.format(test_acc * 100))\n",
    "\n",
    "    print('Train recall: ', recall_score(y_train, train_pred, average='macro'))\n",
    "    print(\"Test recall:\", recall_score(y_test, test_pred, average='macro'), '\\n')\n",
    "    \n",
    "    print(\"Test f1:\", f1_score(y_test, test_pred, average='macro'))\n",
    "    print(\"Test precison : \", precision_score(y_test, test_pred, average='weighted'))\n",
    "    \n",
    "    style.use('fivethirtyeight')\n",
    "    sns.set(style='whitegrid', color_codes=True, font_scale=1.2)   # 'font_scale' = size of label\n",
    "    \n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(test_matrics, annot=True, annot_kws={\"size\": 17}, fmt = \"d\")\n",
    "    plt.show()\n",
    "\n",
    "    return test_acc\n",
    "\n",
    "    \n",
    "def save_model(model, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "        print(\"model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************** Decision TreeFinal ******************\n",
    "\n",
    "name = \"Decision Tree\"\n",
    "dt_estimator = DecisionTreeClassifier(criterion='entropy', max_depth=15, max_features=8,\n",
    "                       random_state=1)\n",
    "dt_model = dt_estimator.fit(X_train, y_train)\n",
    "\n",
    "dt_acc = test(dt_model, name)\n",
    "# save_model(dt_model, 'model_decision_tree.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************* Logistic Regression Final *****************************\n",
    "\n",
    "name = \"Logistic Regression\"\n",
    "lr_estimetor = LogisticRegression(C=0.6, solver='liblinear')\n",
    "\n",
    "lr_model = lr_estimetor.fit(X_train, y_train)\n",
    "\n",
    "lr_acc = test(lr_model, name)\n",
    "# save_model(lr_model, 'model_logistic_regression.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************** SVM Final ******************\n",
    "\n",
    "name = \"SVM\"\n",
    "svm_estimetor = SVC(C=0.0001, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma=7, kernel='linear',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "\n",
    "svm_model = svm_estimetor.fit(X_train, y_train)\n",
    "\n",
    "svm_acc = test(svm_model, name)\n",
    "# save_model(svm_model, 'model_svm.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************** Random Forest Final ******************\n",
    "\n",
    "name = \"Random Forest\"\n",
    "rf_estimetor = RandomForestClassifier(max_depth=10, min_samples_leaf=4, n_estimators=94,\n",
    "                       random_state=137)\n",
    "\n",
    "rf_model = rf_estimetor.fit(X_train, y_train)\n",
    "\n",
    "rf_acc = test(rf_model, name)\n",
    "# save_model(rf_model, 'model_random_forest.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************** xgBoost Final ******************\n",
    "\n",
    "name = \"xgBoost\"\n",
    "xgb_estimetor = XGBClassifier(base_score=0.5, booster=None, class_weight='balanced',\n",
    "        colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=-1,\n",
    "        importance_type='gain', interaction_constraints=None, learning_rate=0.1, max_delta_step=0,\n",
    "        max_depth=10, min_child_weight=1, monotone_constraints=None, n_estimators=50, n_jobs=1,\n",
    "        nthread=1, num_parallel_tree=1, objective='multi:softprob', random_state=0, reg_alpha=0,\n",
    "        reg_lambda=1, scale_pos_weight=None, silent=True, subsample=0.5, tree_method=None,\n",
    "        validate_parameters=False, verbosity=None)\n",
    "\n",
    "xgb_model = xgb_estimetor.fit(X_train, y_train)\n",
    "\n",
    "xgb_acc = test(xgb_model, name)\n",
    "# save_model(rf_model, 'model_xgBoost.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************** Gradient Boosting MachineFinal ******************\n",
    "\n",
    "name = \"Gradient Boosting Machine\"\n",
    "gbm_estimetor = GradientBoostingClassifier(learning_rate=0.2, max_depth=2, n_estimators=120)\n",
    "\n",
    "gbm_model = gbm_estimetor.fit(X_train, y_train)\n",
    "\n",
    "gbm_acc = test(gbm_model, name)\n",
    "# save_model(gbm_model, 'model_gradient_boosting_machine.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************** ADABoost Final ******************\n",
    "\n",
    "name = \"ADABoost\"\n",
    "adab_estimetor = AdaBoostClassifier(n_estimators=120, random_state=10)\n",
    "\n",
    "adab_model = adab_estimetor.fit(X_train, y_train)\n",
    "\n",
    "adab_acc = test(adab_model, name)\n",
    "# save_model(gbm_model, 'model_gradient_boosting_machine.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************** Naive Bayes Final ******************\n",
    "\n",
    "name = \"Naive Bayes\"\n",
    "nb_estimetor = GaussianNB(var_smoothing=0.01)\n",
    "\n",
    "nb_model = nb_estimetor.fit(X_train, y_train)\n",
    "\n",
    "nb_acc = test(nb_model, name)\n",
    "# save_model(knn_model, 'model_knn.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************** KNN Final ******************\n",
    "\n",
    "name = \"KNN\"\n",
    "knn_estimetor = KNeighborsClassifier(metric='manhattan', n_neighbors=17)\n",
    "\n",
    "knn_model = knn_estimetor.fit(X_train, y_train)\n",
    "\n",
    "knn_acc = test(knn_model, name)\n",
    "# save_model(knn_model, 'model_knn.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot...........................................................................................\n",
    "\n",
    "height = [dt_acc*100, lr_acc*100, rn_acc*100, nb_acc*100, svm_acc*100, xgb_acc*100, adab_acc*100, gbm_acc*100]\n",
    "\n",
    "bars = ('Decision tree', 'Logistic', 'Random', 'Naive Bayes','SVM', 'xGBoost', 'ADABoost', 'GradientBoost')\n",
    "\n",
    "y_pos = np.arange(len(bars))\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.bar(y_pos, height, color=['red', 'pink', 'palegreen', 'green', 'blue', 'yellow', 'brown', 'magenta'])\n",
    "plt.xticks(y_pos, bars)\n",
    "plt.title('Accuracy Comparision', size = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning process section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tuning(grid_tree.best_estimator_):\n",
    "    train_pred = grid_tree.best_estimator_.predict(X_train)\n",
    "    y_pred = grid_tree.best_estimator_.predict(X_test)\n",
    "\n",
    "    print('Train Accuracy: ', accuracy_score(y_train, train_pred ))\n",
    "    print(\"Test Accuracy:\", accuracy_score(y_test, y_pred), '\\n')\n",
    "    print('Train recall: ', recall_score(y_train, train_pred, average='macro'))\n",
    "    print(\"Test recall:\", recall_score(y_test, y_pred, average='macro'), '\\n')\n",
    "    print(\"Test f1:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree - OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# creating our parameters to test\n",
    "param_dict={'max_depth': [14,15,17], 'min_samples_leaf':range(1,30,2), 'criterion': ['gini','entropy'],\n",
    "           'max_features': range(0,9)}\n",
    "\n",
    "#create the instance of GridSearchCV using the recall metric for our scoring. \n",
    "grid_tree= GridSearchCV(decision_tree, param_dict, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "#fit the Gridsearch to our data\n",
    "grid_tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_tree.best_score_)\n",
    "print(grid_tree.best_params_)\n",
    "print(grid_tree.best_estimator_)\n",
    "print(\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tuning(grid_tree.best_estimator_)\n",
    "\n",
    "model_dt = grid_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression*****************************\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    lr = LogisticRegression()  # instantiate the model object\n",
    "    #'newton-cg', 'lbfgs', 'liblinear'', 'sag', 'saga'\n",
    "    param_dict={'solver':[ 'lbfgs', 'liblinear', 'sag', 'saga'], 'C': [0.5, 0.6, 0.7], \n",
    "               'penalty': ['l1', 'l2', 'elasticnet']}  # set parameters to be searched through GridSearch\n",
    "\n",
    "    #create the instance of GridSearchCV using the recall metric for our scoring. \n",
    "    grid_tree= GridSearchCV(lr, param_dict, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    #fit the Gridsearch to our data\n",
    "    grid_tree.fit(X_train,y_train)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_tree.best_score_)  # outputs the best metric score\n",
    "print(grid_tree.best_params_)  # outputs the parameters with best result\n",
    "print(grid_tree.best_estimator_)  # outputs the model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tuning(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM ******************************** 'sigmoid', 'precomputed'\n",
    "\n",
    "svm = SVC()\n",
    "# defining parameter range \n",
    "param_grid = {'C': [0.0001, 0.001, 0.01], 'gamma': [8,7,6], 'kernel': ['rbf','linear','poly']}\n",
    "\n",
    "grid_tree=GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "svm_model = grid_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_tree.best_score_)\n",
    "print(grid_tree.best_params_)\n",
    "print(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tuning(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcc = RandomForestClassifier(random_state=137)\n",
    "param_grid = {'max_depth':[8,9,10], 'n_estimators': [90,94,97 ], \n",
    "              'max_features': ['auto', 'log2', 'sqrt'], 'criterion': [ 'gini', 'entropy'],\n",
    "              'min_samples_split': [ 2,3,4,5],'min_samples_leaf': [4,5,8]}\n",
    "\n",
    "grid_tree=GridSearchCV(rfcc, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "model = grid_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_tree.best_score_)\n",
    "print(grid_tree.best_params_)\n",
    "print(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tuning(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report: \\n\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgBoost - OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(class_weight='balanced', objective='binary:logistic',silent=True, nthread=1)\n",
    "# A parameter grid for XGBoost\n",
    "param_grid = {\n",
    "        'min_child_weight': [1], 'subsample': [0.5],'colsample_bytree': [0.8],\n",
    "        'max_depth': [10], 'learning_rate': [0.1],'n_estimators':[50],\n",
    "        # n_estimators=10, learning_rate= 0.02, 'gamma': [ 1],\n",
    "        }\n",
    "grid_tree=GridSearchCV(xgb, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_tree.best_score_)\n",
    "print(grid_tree.best_params_)\n",
    "print(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tuning(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machine - OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "param_grid = [{'n_estimators': [120], 'learning_rate':[0.2],'max_depth': [2],\n",
    "                   'min_samples_split': [2]}]\n",
    "\n",
    "grid_tree=GridSearchCV(gb, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_tree.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_tree.best_score_)\n",
    "print(grid_tree.best_params_)\n",
    "print(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tuning(grid_tree.best_estimator_)\n",
    "gb = grid_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADABoost - OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ada = AdaBoostClassifier()\n",
    "param_grid = {'n_estimators': [100,120], 'algorithm': ['SAMME','SAMME.R'],\n",
    "              'learning_rate': [1.0], 'random_state':[10,20]}\n",
    "grid_tree=GridSearchCV(ada, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_tree.best_score_)\n",
    "print(grid_tree.best_params_)\n",
    "print(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tuning(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes - OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    gnb = GaussianNB()\n",
    "    # defining parameter range \n",
    "    param_grid = {'var_smoothing':[0.01,0.1,0.2]}\n",
    "    grid_tree=GridSearchCV(gnb, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    grid_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_tree.best_score_)\n",
    "print(grid_tree.best_params_)\n",
    "print(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_tuning(grid_tree.best_estimator_)\n",
    "nb= grid_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN - OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "param_dict = {'n_neighbors': range(3,30, 2), 'metric': ['manhattan','euclidean','minkowski'],\n",
    "              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "grid_tree= GridSearchCV(knn, param_dict, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_tree.best_score_)\n",
    "print(grid_tree.best_params_)\n",
    "print(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tuning(grid_tree.best_estimator_)\n",
    "kn= grid_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
